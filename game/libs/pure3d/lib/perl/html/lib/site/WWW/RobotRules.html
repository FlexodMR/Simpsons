    <HTML> 
	<HEAD> 
	    <TITLE>WWW::RobotsRules - Parse robots.txt files

</TITLE> 
	</HEAD>

	<BODY>

<!-- INDEX BEGIN -->

<UL>

	<LI><A HREF="#NAME">NAME</A>
	<LI><A HREF="#SYNOPSIS">SYNOPSIS</A>
	<LI><A HREF="#DESCRIPTION">DESCRIPTION</A>
	<LI><A HREF="#METHODS">METHODS</A>
	<UL>

		<LI><A HREF="#_rules_new_WWW_RobotRules_MO">$rules = new WWW::RobotRules 'MOMspider/1.0'</A>
		<LI><A HREF="#_rules_parse_url_content_f">$rules->parse($url, $content, $fresh_until)</A>
		<LI><A HREF="#_rules_allowed_url_">$rules->allowed($url)</A>
		<LI><A HREF="#_rules_agent_name_">$rules->agent([$name])</A>
	</UL>

	<LI><A HREF="#ROBOTS_TXT">ROBOTS.TXT</A>
	<UL>

		<LI><A HREF="#Examples">Examples</A>
	</UL>

	<LI><A HREF="#SEE_ALSO">SEE ALSO</A>
</UL>
<!-- INDEX END -->

<HR>
<P>
<H1><A NAME="NAME">NAME

</A></H1>
WWW::RobotsRules - Parse robots.txt files


<P>

<P>
<HR>
<H1><A NAME="SYNOPSIS">SYNOPSIS

</A></H1>
<PRE> require WWW::RobotRules;
 my $robotsrules = new WWW::RobotRules 'MOMspider/1.0';
</PRE>

<P>

<PRE> use LWP::Simple qw(get);
</PRE>

<P>

<PRE> $url = &quot;<A HREF="http://some.place/robots.txt&quot">http://some.place/robots.txt&quot</A>;;
 my $robots_txt = get $url;
 $robotsrules-&gt;parse($url, $robots_txt);
</PRE>

<P>

<PRE> $url = &quot;<A HREF="http://some.other.place/robots.txt&quot">http://some.other.place/robots.txt&quot</A>;;
 my $robots_txt = get $url;
 $robotsrules-&gt;parse($url, $robots_txt);
</PRE>

<P>

<PRE> # Now we are able to check if a URL is valid for those servers that
 # we have obtained and parsed &quot;robots.txt&quot; files for.
 if($robotsrules-&gt;allowed($url)) {
     $c = get $url;
     ...
 }
</PRE>

<P>

<P>
<HR>
<H1><A NAME="DESCRIPTION">DESCRIPTION

</A></H1>
This module parses a <EM>robots.txt</EM> file as specified in 
<FONT SIZE=-1>``A</FONT> Standard for Robot Exclusion'', described in &lt;URL:http://info.webcrawler.com/mak/projects/robots/norobots.html&gt;
Webmasters can use the <EM>robots.txt</EM> file to disallow conforming robots access to parts of their 
<FONT SIZE=-1>WWW</FONT> server.


<P>

The parsed file is kept in the WWW::RobotRules object, and this object provide methods to check if access to a given 
<FONT SIZE=-1>URL</FONT> is prohibited. The same WWW::RobotRules object can parse multiple
 <EM>robots.txt</EM> files.


<P>

<P>
<HR>
<H1><A NAME="METHODS">METHODS

</A></H1>
<P>
<HR>
<H2><A NAME="_rules_new_WWW_RobotRules_MO">$rules = new WWW::RobotRules 'MOMspider/1.0'

</A></H2>
This is the constructor for WWW::RobotRules objects. The first argument
given to <CODE>new()</CODE> is the name of the robot. 


<P>

<P>
<HR>
<H2><A NAME="_rules_parse_url_content_f">$rules->parse($url, $content, $fresh_until)

</A></H2>
The <CODE>parse()</CODE> method takes as arguments the 
<FONT SIZE=-1>URL</FONT> that was used to retrieve the <EM>/robots.txt</EM> file, and the contents of the file.


<P>

<P>
<HR>
<H2><A NAME="_rules_allowed_url_">$rules->allowed($url)

</A></H2>
Returns 
<FONT SIZE=-1>TRUE</FONT> if this robot is allowed to retrieve this 
<FONT SIZE=-1>URL.</FONT>



<P>

<P>
<HR>
<H2><A NAME="_rules_agent_name_">$rules->agent([$name])

</A></H2>
Get/set the agent name. 
<FONT SIZE=-1>NOTE:</FONT> Changing the agent name will clear the
robots.txt rules and expire times out of the cache.


<P>

<P>
<HR>
<H1><A NAME="ROBOTS_TXT">ROBOTS.TXT

</A></H1>
The format and semantics of the ``/robots.txt'' file are as follows (this
is an edited abstract of
&lt;URL:http://info.webcrawler.com/mak/projects/robots/norobots.html&gt;):


<P>

The file consists of one or more records separated by one or more blank
lines. Each record contains lines of the form


<P>

<PRE>  &lt;field-name&gt;: &lt;value&gt;
</PRE>

<P>

The field name is case insensitive. Text after the '#' character on a line
is ignored during parsing. This is used for comments. The following
&lt;field-names&gt; can be used:


<P>

<DL>
<DT><STRONG><A NAME="item_User">User-Agent

</A></STRONG><DD>
The value of this field is the name of the robot the record is describing
access policy for. If more than one <EM>User-Agent</EM> field is present the record describes an identical access policy for more
than one robot. At least one field needs to be present per record. If the
value is '*', the record describes the default access policy for any robot
that has not not matched any of the other records.


<P>

<DT><STRONG><A NAME="item_Disallow">Disallow

</A></STRONG><DD>
The value of this field specifies a partial 
<FONT SIZE=-1>URL</FONT> that is not to be visited. This can be a full path, or a partial path; any 
<FONT SIZE=-1>URL</FONT> that starts with this value will not be retrieved



<P>

</DL>
<P>
<HR>
<H2><A NAME="Examples">Examples

</A></H2>
The following example ``/robots.txt'' file specifies that no robots should visit any 
<FONT SIZE=-1>URL</FONT> starting with ``/cyberworld/map/'' or ``/tmp/'':



<P>

<PRE>  # robots.txt for <A HREF="http://www.site.com/">http://www.site.com/</A>
</PRE>

<P>

<PRE>  User-agent: *
  Disallow: /cyberworld/map/ # This is an infinite virtual URL space
  Disallow: /tmp/ # these will soon disappear
</PRE>

<P>

This example ``/robots.txt'' file specifies that no robots should visit any 
<FONT SIZE=-1>URL</FONT> starting with ``/cyberworld/map/'', except the robot called ``cybermapper'':



<P>

<PRE>  # robots.txt for <A HREF="http://www.site.com/">http://www.site.com/</A>
</PRE>

<P>

<PRE>  User-agent: *
  Disallow: /cyberworld/map/ # This is an infinite virtual URL space
</PRE>

<P>

<PRE>  # Cybermapper knows where to go.
  User-agent: cybermapper
  Disallow:
</PRE>

<P>

This example indicates that no robots should visit this site further:


<P>

<PRE>  # go away
  User-agent: *
  Disallow: /
</PRE>

<P>

<P>
<HR>
<H1><A NAME="SEE_ALSO">SEE ALSO

</A></H1>
<A HREF="/n|/perl/html/./lib/site/LWP/RobotUA.html">RobotUA</A>, <A HREF="/n|/perl/html/./lib/site/WWW/RobotRules_AnyDBM_File.html">RobotRules::AnyDBM_File</A>




<P>

</DL>
    </BODY>

    </HTML>
